{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NahinAlam001/499B/blob/UNet/UNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QHW4KhsK3mHe",
        "outputId": "32c1e61d-2401-4197-ab80-31f93f86345a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ISIC-2017\n",
        "!gdown 'https://drive.google.com/uc?export=download&id=1iwCXnS9u6OHD7ENFN76kCrZ7Uu98SUxv'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zoyGE2b17ZaB",
        "outputId": "1f961d4f-14fe-4d68-86df-876d14a614f6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?export=download&id=1iwCXnS9u6OHD7ENFN76kCrZ7Uu98SUxv\n",
            "From (redirected): https://drive.google.com/uc?export=download&id=1iwCXnS9u6OHD7ENFN76kCrZ7Uu98SUxv&confirm=t&uuid=106cb866-d47d-4ace-a590-eeda9a4981b6\n",
            "To: /content/CSE499.zip\n",
            "100% 13.0G/13.0G [04:17<00:00, 50.6MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "with zipfile.ZipFile('CSE499.zip', 'r') as zip_ref:\n",
        "  zip_ref.extractall()"
      ],
      "metadata": {
        "id": "qbC83Sio7nFE"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf sample_data\n",
        "!rm CSE499.zip ISIC-2017_Test_v2_Part3_GroundTruth.csv ISIC-2017_Validation_Part3_GroundTruth.csv"
      ],
      "metadata": {
        "id": "xOB37MrT9zIY"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##train.py"
      ],
      "metadata": {
        "id": "WnyLi0atDJHY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "source_dirs = [\n",
        "    \"/content/ISIC-2017_Training_Data/ISIC-2017_Training_Data\",\n",
        "    \"/content/ISIC-2017_Test_v2_Data/ISIC-2017_Test_v2_Data\",\n",
        "    \"/content/ISIC-2017_Validation_Data/ISIC-2017_Validation_Data\"\n",
        "]\n",
        "\n",
        "destination_dir = \"/content/isic-challenge-2017/ISIC2017_Task1-2_Training_Input\"\n",
        "\n",
        "if not os.path.isdir(destination_dir):\n",
        "  os.makedirs(destination_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "def copy_jpg_files(source_dir, destination_dir):\n",
        "    for root, _, files in os.walk(source_dir):\n",
        "        subdirectory = os.path.relpath(root, source_dir)\n",
        "        destination_subdirectory = os.path.join(destination_dir, subdirectory)\n",
        "        os.makedirs(destination_subdirectory, exist_ok=True)\n",
        "\n",
        "        for filename in files:\n",
        "            if filename.lower().endswith(\".jpg\"):\n",
        "                source_file = os.path.join(root, filename)\n",
        "                destination_file = os.path.join(destination_subdirectory, filename)\n",
        "                shutil.copy2(source_file, destination_file)\n",
        "\n",
        "\n",
        "for source_dir in source_dirs:\n",
        "    copy_jpg_files(source_dir, destination_dir)\n",
        "\n",
        "print(\"JPG files copied successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8alK3nXWE_FU",
        "outputId": "80f78de6-01cd-4f6f-ba1f-96aeee77e52f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "JPG files copied successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def walk_through_dir(path):\n",
        "  for dirpath, _, files in os.walk(path):\n",
        "    if len(files )!=0:\n",
        "      print(f'There are {len(files)} in {dirpath}')\n",
        "walk_through_dir(destination_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NCGhI1XYHdQa",
        "outputId": "906ef8db-6609-4919-8ebb-027adddcabe3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 2750 in /content/isic-challenge-2017/ISIC2017_Task1-2_Training_Input\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "source_dirs = [\n",
        "    \"/content/ISIC-2017_Training_Part1_GroundTruth/ISIC-2017_Training_Part1_GroundTruth\",\n",
        "    \"/content/ISIC-2017_Training_Part2_GroundTruth/ISIC-2017_Training_Part2_GroundTruth\",\n",
        "    \"/content/ISIC-2017_Test_v2_Part1_GroundTruth/ISIC-2017_Test_v2_Part1_GroundTruth\",\n",
        "    \"/content/ISIC-2017_Test_v2_Part2_GroundTruth/ISIC-2017_Test_v2_Part2_GroundTruth\",\n",
        "    \"/content/ISIC-2017_Validation_Part1_GroundTruth/ISIC-2017_Validation_Part1_GroundTruth\",\n",
        "    \"/content/ISIC-2017_Validation_Part2_GroundTruth/ISIC-2017_Validation_Part2_GroundTruth\"\n",
        "]\n",
        "\n",
        "destination_dir = \"/content/isic-challenge-2017/ISIC2017_Task1-2_Training_GroundTruth\"\n",
        "\n",
        "if not os.path.isdir(destination_dir):\n",
        "  os.makedirs(destination_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "def copy_jpg_files(source_dir, destination_dir):\n",
        "    for root, _, files in os.walk(source_dir):\n",
        "        subdirectory = os.path.relpath(root, source_dir)\n",
        "        destination_subdirectory = os.path.join(destination_dir, subdirectory)\n",
        "        os.makedirs(destination_subdirectory, exist_ok=True)\n",
        "\n",
        "        for filename in files:\n",
        "            if filename.lower().endswith(\".png\"):\n",
        "                source_file = os.path.join(root, filename)\n",
        "                destination_file = os.path.join(destination_subdirectory, filename)\n",
        "                shutil.copy2(source_file, destination_file)\n",
        "\n",
        "\n",
        "for source_dir in source_dirs:\n",
        "    copy_jpg_files(source_dir, destination_dir)\n",
        "\n",
        "print(\"PNG files copied successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Add6HZ4IGuT",
        "outputId": "504622be-4c96-4c82-c894-6e14e270d9fc"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PNG files copied successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "walk_through_dir(destination_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9hDS4YpnKol3",
        "outputId": "cbaec3fc-0c31-4540-d5e0-7a1c819c5674"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 2750 in /content/isic-challenge-2017/ISIC2017_Task1-2_Training_GroundTruth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "from glob import glob\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from model import UNet\n",
        "\n",
        "H, W = 256, 256\n",
        "\n",
        "def create_dir(path):\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "\n",
        "class ISICDataset(Dataset):\n",
        "    def __init__(self, images, masks, transform=None):\n",
        "        self.images = images\n",
        "        self.masks = masks\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = cv2.imread(self.images[idx], cv2.IMREAD_COLOR)\n",
        "        image = cv2.resize(image, (W, H)) / 255.0\n",
        "        mask = cv2.imread(self.masks[idx], cv2.IMREAD_GRAYSCALE)\n",
        "        mask = cv2.resize(mask, (W, H)) / 255.0\n",
        "        mask = np.expand_dims(mask, axis=-1)\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image).float()\n",
        "            mask = self.transform(mask).float()\n",
        "\n",
        "        return image, mask\n",
        "\n",
        "def load_data(dataset_path, split=0.2):\n",
        "    images = sorted(glob(os.path.join(dataset_path, \"ISIC2017_Task1-2_Training_Input\", \"*.jpg\")))\n",
        "    masks = sorted(glob(os.path.join(dataset_path, \"ISIC2017_Task1-2_Training_GroundTruth\", \"*.png\")))\n",
        "\n",
        "    if not images or not masks:\n",
        "        raise ValueError(\"No image or mask files found in the specified directories.\")\n",
        "\n",
        "    train_x, test_x = train_test_split(images, test_size=split, random_state=42)\n",
        "    train_y, test_y = train_test_split(masks, test_size=split, random_state=42)\n",
        "    train_x, valid_x = train_test_split(train_x, test_size=split, random_state=42)\n",
        "    train_y, valid_y = train_test_split(train_y, test_size=split, random_state=42)\n",
        "\n",
        "    return (train_x, train_y), (valid_x, valid_y), (test_x, test_y)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    np.random.seed(42)\n",
        "    torch.manual_seed(42)\n",
        "\n",
        "    create_dir(\"files\")\n",
        "\n",
        "    batch_size = 4\n",
        "    lr = 1e-4\n",
        "    num_epochs = 10\n",
        "    model_path = \"/content/drive/MyDrive/499B/ISIC2017_new.pth\"\n",
        "\n",
        "    dataset_path = \"/content/isic-challenge-2017\"\n",
        "    (train_x, train_y), (valid_x, valid_y), (test_x, test_y) = load_data(dataset_path)\n",
        "\n",
        "    transform = transforms.ToTensor()\n",
        "\n",
        "    train_dataset = ISICDataset(train_x, train_y, transform=transform)\n",
        "    valid_dataset = ISICDataset(valid_x, valid_y, transform=transform)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "    valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = UNet(in_channels=3, out_channels=1).to(device)\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        for images, masks in train_loader:\n",
        "            images, masks = images.to(device), masks.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, masks)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        model.eval()\n",
        "        valid_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for images, masks in valid_loader:\n",
        "                images, masks = images.to(device), masks.to(device)\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, masks)\n",
        "                valid_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss/len(train_loader)}, Valid Loss: {valid_loss/len(valid_loader)}\")\n",
        "\n",
        "        torch.save(model.state_dict(), model_path)"
      ],
      "metadata": {
        "id": "VjhAXMFCDQFV"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##eval.py"
      ],
      "metadata": {
        "id": "7ADgP-N52RPd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone -b UNet https://github.com/NahinAlam001/499B.git\n",
        "\n",
        "import shutil\n",
        "\n",
        "filenames = [\"metrics.py\", \"train.py\", \"model.py\"]\n",
        "\n",
        "for filename in filenames:\n",
        "  source = f\"499B/{filename}\"\n",
        "  destination = filename\n",
        "  shutil.copy2(source, destination)\n",
        "  print(f\"Copied {filename} from downloaded branch.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UT4iEEKM62o5",
        "outputId": "c823e48e-65ba-46e2-aa16-5bf6894eecfb"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path '499B' already exists and is not an empty directory.\n",
            "Copied metrics.py from downloaded branch.\n",
            "Copied train.py from downloaded branch.\n",
            "Copied model.py from downloaded branch.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "import pandas as pd\n",
        "from glob import glob\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import accuracy_score, f1_score, jaccard_score, precision_score, recall_score\n",
        "from metrics import dice_loss, dice_coef, iou\n",
        "from train import load_data, create_dir\n",
        "from model import UNet\n",
        "\n",
        "H, W = 256, 256\n",
        "\n",
        "def read_image(path):\n",
        "    x = cv2.imread(path, cv2.IMREAD_COLOR)  ## (H, W, 3)\n",
        "    x = cv2.resize(x, (W, H))\n",
        "    ori_x = x\n",
        "    x = x / 255.0\n",
        "    x = x.astype(np.float32)\n",
        "    x = np.transpose(x, (2, 0, 1))  ## Convert to (3, 256, 256)\n",
        "    x = np.expand_dims(x, axis=0)   ## Add batch dimension\n",
        "    x = torch.tensor(x, dtype=torch.float32)\n",
        "    return ori_x, x  ## (1, 3, 256, 256)\n",
        "\n",
        "def read_mask(path):\n",
        "    x = cv2.imread(path, cv2.IMREAD_GRAYSCALE)  ## (H, W)\n",
        "    x = cv2.resize(x, (W, H))\n",
        "    ori_x = x\n",
        "    x = x / 255.0\n",
        "    x = x.astype(np.int32)  ## (256, 256)\n",
        "    x = torch.tensor(x, dtype=torch.int64)\n",
        "    return ori_x, x\n",
        "\n",
        "def save_results(ori_x, ori_y, y_pred, save_image_path):\n",
        "    line = np.ones((H, 10, 3)) * 255\n",
        "\n",
        "    ori_y = np.expand_dims(ori_y, axis=-1)  ## (256, 256, 1)\n",
        "    ori_y = np.concatenate([ori_y, ori_y, ori_y], axis=-1) ## (256, 256, 3)\n",
        "\n",
        "    y_pred = np.expand_dims(y_pred, axis=-1)  ## (256, 256, 1)\n",
        "    y_pred = np.concatenate([y_pred, y_pred, y_pred], axis=-1) ## (256, 256, 3)\n",
        "\n",
        "    cat_images = np.concatenate([ori_x, line, ori_y, line, y_pred * 255], axis=1)\n",
        "    cv2.imwrite(save_image_path, cat_images)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    \"\"\" Seeding \"\"\"\n",
        "    np.random.seed(42)\n",
        "    torch.manual_seed(42)\n",
        "\n",
        "    \"\"\" Folder for saving results \"\"\"\n",
        "    create_dir(\"results\")\n",
        "\n",
        "    \"\"\" Load the model \"\"\"\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = UNet(in_channels=3, out_channels=1).to(device)\n",
        "    model.load_state_dict(torch.load(\"/content/drive/MyDrive/499B/ISIC2017.pth\", map_location=device))\n",
        "    model.eval()\n",
        "\n",
        "    \"\"\" Load the test data \"\"\"\n",
        "    dataset_path = \"/content/isic-challenge-2017\"\n",
        "    (train_x, train_y), (valid_x, valid_y), (test_x, test_y) = load_data(dataset_path)\n",
        "\n",
        "    SCORE = []\n",
        "    with torch.no_grad():\n",
        "        for x, y in tqdm(zip(test_x, test_y), total=len(test_x)):\n",
        "            \"\"\" Extracting the image name \"\"\"\n",
        "            name = os.path.basename(x)\n",
        "\n",
        "            \"\"\" Read the image and mask \"\"\"\n",
        "            ori_x, x = read_image(x)\n",
        "            ori_y, y = read_mask(y)\n",
        "\n",
        "            \"\"\" Predicting the mask \"\"\"\n",
        "            x = x.to(device)\n",
        "            y_pred = model(x)\n",
        "            y_pred = torch.sigmoid(y_pred)\n",
        "            y_pred = (y_pred > 0.5).float()\n",
        "            y_pred = y_pred.cpu().numpy()[0, 0]  ## Remove batch and channel dimensions\n",
        "            y_pred = y_pred.astype(np.int32)\n",
        "\n",
        "            \"\"\" Saving the predicted mask \"\"\"\n",
        "            save_image_path = f\"results/{name}\"\n",
        "            save_results(ori_x, ori_y, y_pred, save_image_path)\n",
        "\n",
        "            \"\"\" Flatten the array \"\"\"\n",
        "            y = y.numpy().flatten()\n",
        "            y_pred = y_pred.flatten()\n",
        "\n",
        "            \"\"\" Calculating metrics values \"\"\"\n",
        "            acc_value = accuracy_score(y, y_pred)\n",
        "            f1_value = f1_score(y, y_pred, labels=[0, 1], average=\"binary\")\n",
        "            jac_value = jaccard_score(y, y_pred, labels=[0, 1], average=\"binary\")\n",
        "            recall_value = recall_score(y, y_pred, labels=[0, 1], average=\"binary\")\n",
        "            precision_value = precision_score(y, y_pred, labels=[0, 1], average=\"binary\")\n",
        "            SCORE.append([name, acc_value, f1_value, jac_value, recall_value, precision_value])\n",
        "\n",
        "    \"\"\" mean metrics values \"\"\"\n",
        "    score = [s[1:] for s in SCORE]\n",
        "    score = np.mean(score, axis=0)\n",
        "    print(f\"\\nAccuracy: {score[0]:0.5f}\")\n",
        "    print(f\"F1: {score[1]:0.5f}\")\n",
        "    print(f\"Jaccard: {score[2]:0.5f}\")\n",
        "    print(f\"Recall: {score[3]:0.5f}\")\n",
        "    print(f\"Precision: {score[4]:0.5f}\")\n",
        "\n",
        "    df = pd.DataFrame(SCORE, columns=[\"Image Name\", \"Acc\", \"F1\", \"Jaccard\", \"Recall\", \"Precision\"])\n",
        "    df.to_csv(\"files/score.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "05vlf6cK2TbD",
        "outputId": "f50b6ca1-c4e1-4079-9a26-3eeb16c9dbe4"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 550/550 [03:22<00:00,  2.71it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.20717\n",
            "F1: 0.30500\n",
            "Jaccard: 0.20717\n",
            "Recall: 1.00000\n",
            "Precision: 0.20717\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}